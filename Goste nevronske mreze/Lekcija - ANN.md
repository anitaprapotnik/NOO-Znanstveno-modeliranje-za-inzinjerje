
# Umetna inteligenca in nevronske mreÅ¾e
Umetna inteligenca je zelo Å¡irok pojem. Evropska unija jo opredeljuje kot Â»sposobnost stroja, da posnema ÄloveÅ¡ke sposobnosti, kot so logiÄno razmiÅ¡ljanje, uÄenje, naÄrtovanje in ustvarjalnost.Â«

Leta 1956 je umetna inteligenca z ustanovitvijo samostojnega raziskovalnega podroÄja postala del znanstvene sfere.

Pojem umetne inteligence zajema najrazliÄnejÅ¡e strojne funkcije in algoritme, ki med seboj niso nujno povezani.

Velik preboj na podroÄju umetne inteligence se je zgodil v osemdesetih letih z razvojem umetnih ***nevronskih mreÅ¾***. Gre za algoritme, ki raÄunalniku omogoÄajo strojno uÄenje iz velike koliÄine podatkov.

Poznamo veÄ vrst nevronskih mreÅ¾, ki jih lahko razvrÅ¡Äamo po razliÄnih merilih â€“ na primer glede na naÄin uÄenja ali vrsto podatkov, ki jih obdelujejo.

Posebna podzvrst nevronskih mreÅ¾, prilagojena razpoznavanju in ustvarjanju besedila, so tako imenovani ***veliki jezikovni modeli*** (large language models), med katerimi je najbolj znan ChatGPT.

ÄŒeprav danes veÄina ljudi ob omembi umetne inteligence najprej pomisli na ChatGPT ali drug veliki jezikovni model, so ti v resnici le ena izmed podkategorij nevronskih mreÅ¾, te pa spadajo le med eno izmed podkategorij umetne inteligence.

V nadaljevanju bomo spoznali princip delovanja nevronskih mreÅ¾ in se nauÄili, kako v Pythonovi knjiÅ¾nici Keras najprej sestaviti osnovno (gosto) nevronsko mreÅ¾o, nato pa Å¡e nevronsko mreÅ¾o za razpoznavanje slik.

# Kako delujejo nevronske mreÅ¾e?
Nevronska mreÅ¾a je raÄunalniÅ¡ki algoritem za strojno uÄenje. Kot Å¾e samo ime pove, je bila inspiracija za razvoj nevronskih mreÅ¾ povzeta po delovanju Å¾ivÄnih celic (nevronov) v ÄloveÅ¡kih moÅ¾ganih.

TipiÄen problem, ki ga nevronske mreÅ¾e reÅ¡ujejo, lahko ponazorimo na naslednjem primeru:

Predstavljajmo si tabelo s petimi stolpci in velikim Å¡tevilom vrstic.
V prvih treh stolpcih (A, B in C) so vneseni neodvisni podatki, v zadnjih dveh stolpcih (D in E) pa podatki, ki so odvisni od vrednosti stolpcev A, B in C.

Osnovna ideja delovanja nevronskih mreÅ¾ je v tem, da raÄunalniku podamo takÅ¡no tabelo in mu omogoÄimo, da se iz podatkov nauÄi, kako sta stolpca D in E odvisna od stolpcev A, B in C.
Ko se mreÅ¾a tega nauÄi, lahko od nje priÄakujemo, da bo ob novih vrednostih stolpcev A, B in C (ki jih prej ni videla) znala napovedati pripadajoÄe vrednosti stolpcev D in E.

Å tevilo neodvisnih in odvisnih stolpcev je seveda poljubno in je odvisno od konkretnega problema.

Nekaj primerov uporabe:

V neodvisne stolpce lahko zapiÅ¡emo osnovne zdravstvene podatke osebe (na primer viÅ¡ino, teÅ¾o, krvni tlak, podatke o krvni sliki, ali je oseba kadilec ali ne â€¦), v odvisne stolpce pa, ali je 
posameznik v Å¾ivljenju zbolel za doloÄeno boleznijo. RaÄunalnik se lahko na podlagi teh podatkov nauÄi napovedovati verjetnost, da bo oseba v prihodnosti zbolela za doloÄeno boleznijo.

Drug primer je podroÄje prometne varnosti.
V neodvisne stolpce lahko zapiÅ¡emo podatke o cesti, kot so gostota prometa, vidljivost, prisotnost in vrsta signalizacije ter geometrijske lastnosti ceste, v odvisni stolpec pa Å¡tevilo prometnih 
nesreÄ na doloÄenem kriÅ¾iÅ¡Äu. RaÄunalnik se lahko na podlagi teh podatkov nauÄi oceniti nevarnost kriÅ¾iÅ¡Äa, torej verjetnost, da na njem pride do nesreÄe.

# Arhitektura nevronske mreÅ¾e
Arhitekturo nevronske mreÅ¾e sestavljajo vozliÅ¡Äa ali nevroni (ang. nodes / neurons), ki so na sliki prikazani s krogi. VozliÅ¡Äa so razporejena v stolpce, ki jih imenujemo plasti (ang. layers).
Prvo plast imenujemo vhodna plast (input layer), zadnjo izhodna plast (output layer), vmesne pa skrite plasti (hidden layers). 
Vsaka nevronska mreÅ¾a ima torej vhodno plast, izhodno plast ter eno ali veÄ skritih plasti.

VozliÅ¡Äa so med seboj povezana s povezavami. Pri osnovni (gosti) arhitekturi je vsako vozliÅ¡Äe v posamezni plasti povezano z vsemi vozliÅ¡Äi v sosednjih plasteh, kot je prikazano na zgornjih 
slikah. Vsaki povezavi je pripisana Å¡tevilÄna vrednost, imenovana uteÅ¾ (ang. weight, na sliki oznaÄena z w). Vsem vozliÅ¡Äem, razen tistim v vhodni plasti, pripiÅ¡emo Å¡e Å¡tevilko, imenovano odmik ali 
pristranskost (ang. bias, na sliki oznaÄeni z b). UteÅ¾i in odmiki so na zaÄetku doloÄeni kot nakljuÄno izÅ¾rebane vrednosti med 0 in 1.

Podatki iz tabele skozi nevronsko mreÅ¾o prehajajo vrstico za vrstico.
Kolikor neodvisnih stolpcev ima tabela, toliko vozliÅ¡Ä mora imeti vhodna plast, saj vsako vozliÅ¡Äe predstavlja podatek enega stolpca.

# Potovanje podatkov skozi nevrosnko mreÅ¾o

Podatki iz vhodne plasti nato potujejo po povezavah do vozliÅ¡Ä v naslednji plasti. Vsaka povezava pomeni mnoÅ¾enje podatka z ustrezno uteÅ¾jo. V vsakem vozliÅ¡Äu se vsi dohodni podatki seÅ¡tejejo,
vsoti pa se priÅ¡teje Å¡e vrednost odmika (ang. bias) tega vozliÅ¡Äa.

Izhodna plast predstavlja podatke, ki jih Å¾elimo napovedati. V izhodni plasti mora biti torej toliko vozliÅ¡Ä, kolikor je izhodnih spremenljivk â€” v naÅ¡em primeru dve vozliÅ¡Äi, ki predstavljata stolpca D in E.

Po prvem prehodu podatkov bodo vrednosti na izhodu seveda popolnoma razliÄne od priÄakovanih. Ideja uÄenja je v tem, da z zaporednim prilagajanjem uteÅ¾i in odmikov mreÅ¾a postopoma izboljÅ¡uje svoje napovedi, 
tako da se izhodni podatki Äim bolj pribliÅ¾ajo priÄakovanim vrednostim.

***Pomembno:***
Nevronska mreÅ¾a mora imeti:

 - v vhodni plasti toliko vozliÅ¡Ä, kolikor je vhodnih (neodvisnih) podatkov,

 - v izhodni plasti toliko vozliÅ¡Ä, kolikor je izhodnih (odvisnih) podatkov, ki jih Å¾elimo napovedati.

Å tevilo skritih plasti in vozliÅ¡Ä v posamezni skriti plasti pa je poljubno in se doloÄi glede na zahtevnost problema.

 # Aktivacijska funkcija
Prej opisani postopek ima eno pomembno pomanjkljivost, ki bi jo matematiki hitro opazili: linearna kombinacija dveh linearnih kombinacij je namreÄ ponovno linearna kombinacija.

Na primeru bomo razloÅ¾ili, kaj to pomeni.
Predpostavimo, da imamo nevronsko mreÅ¾o z naslednjo arhitekturo: v vhodni plasti sta dve vozliÅ¡Äi, v skriti plasti prav tako dve vozliÅ¡Äi, 
v izhodni plasti pa eno vozliÅ¡Äe. UteÅ¾i med prvo in drugo plastjo oznaÄimo z $w_1$, uteÅ¾i med drugo in tretjo plastjo pa z $w_2$.

Vrednosti vozliÅ¡Ä v skriti plasti sta torej:

$a^1=w^1_{1,1}I_1+w^2_{1,2}I_2+b_1$   in $a^2=w^1_{2,1}I_1+w^1_{2,2}I_2+b_2$.

Vrednost v izhodnem vozliÅ¡Äu pa je torej:

$o=w^2_{1,1}a^1+w^2_{1,2}a_2+b_0=w^2_{1,1}(w^1_{1,1}I_1+w^1_{1,2}I_2+b_1)+w^2_{1,2}(w^1_{2,1}I_1+w^1_{2,2}I_2+b_2)=$

$(w^2_{1,1}w^1_{1,1}+w^2_{1,2}w^1_{2,1})I_1+(w^2_{1,1}w^1_{1,2}+w^2_{1,2}w^1_{2,2})I_2+(w^2_{1,1}b_1+w^2_{1,2}b_2+b_0)=w_1I_1+w_2I_2+b$

Vidimo torej, da je skrita plast brez pomena, saj bi z ustrezno izbiro uteÅ¾i enak rezultat dobili Å¾e, Äe bi vhodno plast neposredno povezali z izhodno.

Da to prepreÄimo, na vrednosti v vsakem vozliÅ¡Äu uporabimo nelinearno funkcijo, ki jo imenujemo aktivacijska funkcija. Najbolj pogosto uporabljane aktivacijske funkcije so zbrane v priloÅ¾eni datoteki tabele.pdf

 # Priprava podatkov
ÄŒeprav nevronske mreÅ¾e lahko obdelujejo razliÄne vrste podatkov, kot so slike, videoposnetki in besedilo, se bomo tukaj osredotoÄili le na pripravo preprostih Å¡tevilskih podatkov in kategorij.

ÄŒe kot vhod v nevronsko mreÅ¾o uporabljamo zvezne podatke, jih je najprej smiselno normalizirati. To pomeni, da podatke preslikamo v interval med 0 in 1 oziroma med â€“1 in 1, da imajo vsi vhodni 
podatki primerljivo velikost in da se izognemo numeriÄnim teÅ¾avam pri uÄenju modela.

ÄŒe pa imamo opravka z diskretnimi podatki â€“ kategorijami (na primer, Å¾elimo razlikovati med psom in maÄko, prepoznati barvo las ali ugotoviti, ali bo oseba zbolela ali ne) â€“ imamo dve moÅ¾nosti, 
kako jih pretvoriti v Å¡tevila.

Prva moÅ¾nost je uporaba celih Å¡tevil, kjer vsako kategorijo oznaÄimo z ustrezno celo Å¡tevilko, na primer:

 rdeÄa â†’ 1, Ärna â†’ 2, rumena â†’ 3.
ÄŒeprav je ta pristop enostaven, ima resno pomanjkljivost â€“ umetno nakazuje zaporedje ali velikostni odnos, ki v kategorijah v resnici ne obstaja (npr. da je rumena â€œveÄâ€ kot Ärna, ali Ärna â€œveÄâ€ kot rdeÄa).

Da se temu izognemo â€“ in s tem tudi teÅ¾avam pri uÄenju modela â€“ uporabljamo t. i. â€œone-hotâ€ notacijo. Pri tem vsako kategorijo predstavimo z vektorjem dolÅ¾ine N, kjer je N Å¡tevilo kategorij.
ÄŒe je na primer moÅ¾en rezultat rdeÄa, modra ali rumena, potem:

rdeÄa â†’ (1, 0, 0)

modra â†’ (0, 1, 0)

rumena â†’ (0, 0, 1)

Cela Å¡tevila pa uporabljamo le takrat, kadar med podatki res obstaja vrstni red ali hierarhija (npr. konÄana osnovna, srednja ali visoka Å¡ola).

# Ocena napake - stroÅ¡kovna funkcija
V naslednjem koraku mora model nevronske mreÅ¾e prilagoditi uteÅ¾i in odmike, da se izhod Äim bolje prilagodi priÄakovanim vrednostim. Da bi to dosegli, potrebujemo najprej mero napake - 
tako imenovano stroÅ¡kovno funkcijo, ki je odvisna od vrste podatkov, s katerimi delamo. Poglejmo, katere napake uporabljamo v primeru zveznih podatkov in v primeru kategorialnih podatkov.
Zvezni podatki

V primeru zveznih podatkov uporabljamo varianco oziroma povpreÄno kvadratno napako.

EnaÄba:

$ğ‘€ğ‘†ğ¸=\frac{1}{ğ‘}\sum^ğ‘_{ğ‘–=0}(ğ‘¦_ğ‘–âˆ’\hat{ğ‘¦}_ğ‘–)^2$,

kjer je $ğ‘¦_ğ‘–$ dejanska vrednost, $\hat{ğ‘¦}_ğ‘–$ napoved modela in N Å¡tevilo vzorcev. 

Opomba: varianca je obÄutljiva na izstopajoÄe vrednosti. ÄŒe Å¾elimo zmanjÅ¡ati to obÄutljivost, lahko izberemo drug naÄin ocenjevanja napake.
Klasifikacija

V primeru klasifikacije, ko je rezultat podan z one-hot zapisom, uporabljamo kriÅ¾no entropijo (ang. cross entropy).

EnaÄba:

$ğ»=âˆ’\sum^N_{ğ‘–=1} \sum^ğ¶_{ğ‘=1} ğ‘¦_{ğ‘–,ğ‘} \log(\hat ğ‘¦_{ğ‘–,ğ‘})$,

kjer je  N Å¡tevilo vzorcev, C Å¡tevilo razredov, $ğ‘¦_{ğ‘–,ğ‘}$ dejanska vrednost in $\hat ğ‘¦_{ğ‘–,ğ‘}$ napoved modela.

Primer:
Recimo, da je pravi razred druga kategorija â€” v one-hot zapisu torej $ğ‘¦=[0,1,0]$.

ÄŒe model napove verjetnosti razredov $\hat ğ‘¦ =[0.7,0.2,0.1]$, ima kriÅ¾na entropija vrednost:

$ğ»=âˆ’[0 \cdot ğ‘™ğ‘œğ‘” (0.7)+1 \cdot ğ‘™ğ‘œğ‘” (0.2)+0\cdot ğ‘™ğ‘œğ‘” (0.1)]=âˆ’ ğ‘™ğ‘œğ‘” (0.2)=1.609$

ÄŒe pa model napove verjetnosti $\hat{y} =[0.1,0.8,0.1]$, dobimo:

$ğ»=âˆ’[0\cdot ğ‘™ğ‘œğ‘” (0.1)+1\cdot ğ‘™ğ‘œğ‘” (0.8)+0\cdot ğ‘™ğ‘œğ‘” (0.1)]=âˆ’ ğ‘™ğ‘œğ‘” (0.8)=0.223$

VeÄja kot je verjetnost, ki jo model pripiÅ¡e pravemu razredu, manjÅ¡a je vrednost kriÅ¾ne entropije.

Druga moÅ¾nost je uporaba redke kriÅ¾ne entropije, ki je matematiÄno enaka kriÅ¾ni entropiji, vendar preverja le verjetnost pravega razreda â€” torej ni potrebno uporabiti celotnega one-hot zapisa.

EnaÄba se poenostavi v:  $ğ»=âˆ’ ğ‘™ğ‘œğ‘” (\hat y)$.

V prvem primeru, ko je napoved $\hat ğ‘¦ =[0.7,0.2,0.1]$, preverjamo samo verjetnost za pravi razred (0.2) in dobimo:

$ğ»=âˆ’ ğ‘™ğ‘œğ‘” (0.2)=1.609$,

Katero moÅ¾nost â€” kriÅ¾no entropijo ali redko kriÅ¾no entropijo â€” bomo uporabili v programu, je odvisno od naÄina, kako imamo podane (zapisane) prave oziroma priÄakovane vrednosti kategorij.

PodrobnejÅ¡i seznam stroÅ¡kovnih funkcij je podan v priloÅ¾eni datoteki tabele.pdf.


# Proces uÄenja - optimizacija
Ko enkrat doloÄimo oceno napake (t. i. stroÅ¡kovno funkcijo), lahko pristopimo k optimizaciji. Osnovna ideja optimizacijskega procesa je poiskati minimum napake. To storimo tako, da izraÄunamo (parcialne) 
odvode napake po uteÅ¾eh in odmikih ter tako dobimo smer, v katero se moramo premakniti, da se pribliÅ¾amo minimumu.



Velikost koraka, s katerim se premikamo v tej smeri, imenujemo hitrost uÄenja (ang. learning rate). Ta mora biti skrbno izbrana: 
premajhen korak pomeni poÄasno uÄenje mreÅ¾e, prevelik korak pa lahko povzroÄi preskakovanje minimuma in s tem nestabilno uÄenje.

Opisano metodo imenujemo gradientni spust (GD â€“ gradient descent). Ker je v svoji osnovni obliki razmeroma poÄasna, v praksi pogosteje uporabljamo njene izboljÅ¡ane razliÄice.

- Metoda nakljuÄnega gradientega spusta (SGD, ang. Stochastic Gradient Descent): Namesto da bi uteÅ¾i prilagodili po prehodu vseh podatkov, jih posodabljamo po vsakem posameznem vzorcu. Metoda omogoÄa hitro uÄenje ij no uporabljamo kadar 
imamo zelo veliko podatkov ali Å¾elimo hitro pribliÅ¾no reÅ¡itev.

 - Mini-batch Gradient Descent: MeÅ¡anica med SGD in GD - namesto enega vzorca ali vseh podatkov izbere vmesno pot in uteÅ¾i posodablja po prehodu skupka vzorcev (takoimenovanega batch-a). 
ZdruÅ¾uje prednosti obeh pristopov â€“ veÄjo stabilnost posodabljanja kot SGD in hitrejÅ¡e uÄenje kot klasiÄni gradientni spust.

 - SGD z zagonom (ang. GSD with mometum). Za smer premika kombinira smer trenutnega odvoda in prejÅ¡njih odvodov - torej nekako ohranja informacijo prejÅ¡njih odvodov. Hitreje premikamo skozi dolge,
ravne doline in zmanjÅ¡amo nihanje v bliÅ¾ini minimuma. Uporablja se pri kompleksnih modelih, kjer je funkcija napake zelo valovita.

 - AdaGrad prilagaja velikost koraka za vsako uteÅ¾ posebej â€” pogoste spremembe dobijo manjÅ¡e korake, redke pa veÄje. Dobro deluje pri redkih podatkih (npr. pri obdelavi besedil), vendar se koraki sÄasoma preveÄ
zmanjÅ¡ajo, kar lahko upoÄasni uÄenje.

 - RMSProp je AdaGrad z momentom: ne premikamo se v smeri zadnejga gradienta ampak v smeri povpreÄne vrednosti nekaj zaporedih gradientov (t.i. tekoÄe povpreÄje ali moving average).
Deluje zelo dobro pri nepostojnih problemih, kot so nevronske mreÅ¾e, kjer se gradienti s Äasom spreminjajo.
 
 - AdaDelta je izboljÅ¡ana RMSProp metoda, ki na pameten naÄin doloÄi zaÄetni korak z uporabo relativnih sprememb. S tem omogoÄa stabilno uÄenje brez natanÄnega nastavljanja parametrov.
 
  - Adam (Adaptive Moment Estimation) â€“ izraÄuna povpreÄje gradientov in njihovih kvadratov, kar omogoÄa hitrejÅ¡e in stabilnejÅ¡e uÄenje. Je ena najpogosteje uporabljenih metod danes, saj dobro deluje v veÄini
primerov brez veliko prilagajanja.

NatanÄnejÅ¡i opisi optimizacijskih funkcij so zbrani v tabele.pdf.

 # UÄenje in validacija
Pri procesu uÄenja nevrosnka mreÅ¾a prejema podatke in glede na izbrano metodo optimizacije prilagaja uteÅ¾i. Ves Äas spremljamo napako (stroÅ¡kovno funkcijo). Ko opazimo, da napaka doseÅ¾e plato in se ne zmanjÅ¡uje, 
lahko sklepamo, da se je model nauÄil.

***Parametri procesa uÄenja: ***

 - Å tevilo epoh uÄenja â€“ doloÄa, kolikokrat model preide skozi celoten nabor podatkov (ena epoha pomeni en prehod podatkov). ObiÄajno ni dovolj, da model podatke "vidi" le enkrat; za uspeÅ¡no uÄenje jih
 moramo veÄkrat prevleÄi skozi mreÅ¾o. Po vsakem prehodu je priporoÄljivo podatke nakljuÄno premeÅ¡ati.

 - Batch size â€“ velikost vzorca, ki ga model obravnava "naenkrat". To je Å¡tevilo vzorcev, ki gre skozi model, preden se uteÅ¾i ponovno prilagodijo.

 - Metrika â€“ naÄin, s katerim merimo natanÄnost modela. Pogosto uporabimo isto metodo kot za stroÅ¡kovno funkcijo, ni pa nujno. Razlika je v tem, da se stroÅ¡kovna funkcija uporablja v procesu uÄenja,
 medtem ko metrika le ocenjuje natanÄnost modela.

Po konÄanem uÄenju je obvezna validacija nauÄenega modela. To pomeni, da podatke, ki jih model vidi prviÄ (na katerih se ni uÄil), spustimo skozi mreÅ¾o. Za validacijo je obiÄajno na zaÄetku del podatkov, 
namenjenih uÄenju, loÄen na stran (pribliÅ¾no 10 % podatkov). Po prehodu teh podatkov ocenimo natanÄnost z izbrano metriko.

VÄasih bo natanÄnost pri validaciji niÅ¾ja kot med uÄenjem. Razlog je, da se nevrosnka mreÅ¾a lahko bolj "napamet nauÄi" rezultate, namesto da bi pogruntala dejanske povezave med podatki. Temu pojavu pravimo overfitting. 
Problem reÅ¡ujemo tako, da prilagodimo arhitekturo (Å¡tevilo plasti, Å¡tevilo vozliÅ¡Ä) in uporabimo funkcijo dropout, ki po prehodu doloÄi del podatkov kot nepomemben.

NatanÄnost natrenirane mreÅ¾e skoraj nikoli ni 100 %. NaÅ¡ cilj je, da se ji Äim bolj pribliÅ¾amo. To doseÅ¾emo s prilagajanjem arhitekture. Pravega pravila ni â€” obiÄajno moramo preizkusiti veÄ moÅ¾nosti in oceniti, 
katera prinese najboljÅ¡e rezultate.

NatanÄnejÅ¡i seznam moÅ¾nih metrik je zbran v tabele.pdf.
